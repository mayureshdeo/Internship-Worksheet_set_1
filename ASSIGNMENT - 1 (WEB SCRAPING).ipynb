{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfad721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n",
      "<span class=\"vector-menu-heading-label\">English</span>\n",
      "<span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
      "<span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-cactions-label\">\n",
      "<span class=\"vector-menu-heading-label\">More</span>\n",
      "<span class=\"vector-menu-checkbox-expanded\">expanded</span>\n",
      "<span class=\"vector-menu-checkbox-collapsed\">collapsed</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 aria-label=\"\" class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed71955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc9085f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5c44d9784013>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;34m\"year\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;34m\"place\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;34m\"star_cast\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcrew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;34m\"rating\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;34m\"vote\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvotes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crew' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Download IMDB's Top 250 data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('td.titleColumn')\n",
    "ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "imdb = []\n",
    "\n",
    "# Store each item into dictionary (data), then put those into a list (imdb)\n",
    "for index in range(0, len(movies)):\n",
    "    # Seperate movie into: 'Name', 'rating', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\": movie_title,\n",
    "            \"year\": year,\n",
    "            \"place\": place,\n",
    "            \"star_cast\": crew[index],\n",
    "            \"rating\": ratings[index],\n",
    "            \"vote\": votes[index],\n",
    "            \"link\": links[index]}\n",
    "    imdb.append(data)\n",
    "\n",
    "for item in imdb:\n",
    "    print(item['place'], '-', item['movie_title'], '('+item['year']+') -', 'Starring:', item['star_cast'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the lists we want to write into\n",
    "titles = []\n",
    "years = []\n",
    "time = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "us_gross = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81166ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting English translated titles from the movies\n",
    "headers = {'Accept-Language': 'en-US, en;q=0.5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e38ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing each of the urls of 50 movies \n",
    "for page in pages:\n",
    "    # Getting the contents from the each url\n",
    "    page = requests.get('https://www.imdb.com/search/title/?groups=top_1000&start=' + str(page) + '&ref_=adv_nxt', headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Aiming the part of the html we want to get the information from\n",
    "    movie_div = soup.find_all('div', class_='lister-item mode-advanced')\n",
    "    \n",
    "    # Controling the loop’s rate by pausing the execution of the loop for a specified amount of time\n",
    "    # Waiting time between requests for a number between 2-10 seconds\n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "    for container in movie_div:\n",
    "# Scraping the movie's name\n",
    "        name = container.h3.a.text\n",
    "        titles.append(name)\n",
    "# Scraping the movie's year\n",
    "        year = container.h3.find('span', class_='lister-item-year').text\n",
    "        years.append(year)\n",
    "        \n",
    "# Scraping the movie's length\n",
    "        runtime = container.find('span', class_='runtime').text if container.p.find('span', class_='runtime') else '-'\n",
    "        time.append(runtime)\n",
    "        \n",
    "# Scraping the rating\n",
    "        imdb = float(container.strong.text)\n",
    "        imdb_ratings.append(imdb)\n",
    "                # Scraping the metascore\n",
    "        m_score = container.find('span', class_='metascore').text if container.find('span', class_='metascore') else '-'\n",
    "        metascores.append(m_score)\n",
    "        \n",
    "# Scraping votes and gross earnings\n",
    "        nv = container.find_all('span', attrs={'name':'nv'})\n",
    "        vote = nv[0].text\n",
    "        votes.append(vote)\n",
    "        grosses = nv[1].text if len(nv) > 1 else '-'\n",
    "        us_gross.append(grosses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad8088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##main modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.espncricinfo.com/icc-cricket-world-cup-2015/content/current/series/509587.html'\n",
    "page=requests.get(url)\n",
    "print(type(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02155413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "urls = [\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/test/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/test/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/mens/player-rankings/t20i/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/batting\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\",\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/t20i/bowling\",\n",
    "]\n",
    "\n",
    "final_result_file_name = \"All Ranking List.csv\"\n",
    "final_column_names = [\"Ranking Type\", \"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "pd.DataFrame(columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "for url in urls:\n",
    "    request_object = requests.get(url, headers=headers)\n",
    "    html_content = request_object.text\n",
    "    print(request_object.status_code, \"->\", url)\n",
    "    soup_object = BeautifulSoup(html_content, \"lxml\")\n",
    "    for element in soup_object.select('[class=\"ranking-pos up\"], [class=\"ranking-pos down\"]'):\n",
    "        element.replace_with(BeautifulSoup(\"<\" + element.name + \"></\" + element.name + \">\", \"html.parser\"))\n",
    "\n",
    "    ranking_type = soup_object.select_one(\".rankings-block__title-container > h4\").text\n",
    "\n",
    "    result_file_name = ranking_type + \".csv\"\n",
    "    column_names = [\"Position\", \"Player Name\", \"Team Name\", \"Rating\", \"Career Best Rating\", \"Crawl URL\"]\n",
    "    pd.DataFrame(columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    for element in soup_object.select('table[class=\"table rankings-table\"] tr'):\n",
    "        if(element.find(\"th\")):\n",
    "            continue\n",
    "        data_dict = dict()\n",
    "        data_dict[\"Crawl URL\"] = url\n",
    "        data_dict[\"Ranking Type\"] = ranking_type\n",
    "        if(element.select_one('[class*=\"position\"]')):\n",
    "            data_dict[\"Position\"] = element.select_one('[class*=\"position\"]').text\n",
    "        for player_name in (element.select('a[href*=\"/player-rankings\"]')):\n",
    "            if(player_name.text.strip()):\n",
    "                data_dict[\"Player Name\"] = player_name.text\n",
    "        if(element.select_one('[class^=\"flag-15\"]')):\n",
    "            data_dict[\"Team Name\"] = element.select_one('[class^=\"flag-15\"]')[\"class\"][-1]\n",
    "        if(element.select_one('[class$=\"rating\"]')):\n",
    "            data_dict[\"Rating\"] = element.select_one('[class$=\"rating\"]').text\n",
    "        if(element.select_one('td.u-hide-phablet')):\n",
    "            data_dict[\"Career Best Rating\"] = element.select_one('td.u-hide-phablet').text\n",
    "        for key in data_dict.keys():\n",
    "            data_dict[key] = re.sub(r\"\\s+\", \" \", data_dict[key])\n",
    "            data_dict[key] = data_dict[key].strip()\n",
    "        pd.DataFrame([data_dict], columns=column_names).to_csv(result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")\n",
    "        pd.DataFrame([data_dict], columns=final_column_names).to_csv(final_result_file_name, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71839cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba80994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web_scraping.py\n",
    "\n",
    "from bs4 import BeautifulSoup as soup  \n",
    "from urllib.request import urlopen \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url = 'https://www.swiggy.com/restaurants/flavours-of-taj-mahal-hotel-abids-road-abids-and-koti-hyderabad-7203'\n",
    "\n",
    "content = urlopen(get_url)\n",
    "page_soup = soup(content.read(),\"html.parser\")\n",
    "content.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = page_soup.findAll(\"div\", attrs={\"class\":\"_2IEoD\"})\n",
    "\n",
    "Food_items=[]  #List to store name of the food items\n",
    "prices=[]      #List to store prices of the food itmes\n",
    "\n",
    "for x in containers:\n",
    "    Recommended = x.find(\"div\",{\"class\":\"_2SyqU\"}) # getting here food item names\n",
    "    price = x.find(\"span\",{\"class\":\"bQEAj\"}) # getting here food prices\n",
    "\n",
    "    Food_items.append(Recommended.text) \n",
    "    prices.append(price.text)\n",
    "    \n",
    "df = pd.DataFrame({'Food Item Name':Food_items,'Food item prices':prices}) \n",
    "df.to_csv('menu.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d467a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30776f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2eea35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8589e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684c72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5247ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3898b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
